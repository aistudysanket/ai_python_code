{"cells":[{"cell_type":"code","execution_count":null,"id":"cf1739e0","metadata":{"id":"cf1739e0"},"outputs":[],"source":["'''\n","This code will walk through the core steps: taking a small text corpus, automatically building a vocabulary from it,\n","and then using that vocabulary to convert text to numbers and back again.\n","\n","How This Code Relates to the Tokenization Guide provided to you as a pdf\n","\n","    Corpus: The corpus list is our small-scale version of the text data mentioned in Step 1.\n","\n","    Strategy & Training: tf.keras.layers.TextVectorization acts as our word-based tokenizer. The .adapt(corpus) method performs Step 2 and 3,\n","     automatically learning the vocabulary from the data.\n","\n","    Special Tokens: When you inspect the vocabulary, you'll see it automatically includes '' (for padding, which maps to 0) and [UNK]\n","    (for unknown words, which maps to 1). This corresponds to Step 4.\n","\n","    Final Tokenizer: The text_vectorizer object is our finalized tokenizer, ready to perform the encoding and decoding tasks described in Step 5.\n","\n","'''"]},{"cell_type":"code","execution_count":null,"id":"780a4497","metadata":{"id":"780a4497","outputId":"536112e7-d97e-495d-9404-8986a93e975c"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Training the tokenizer... ---\n","Vocabulary built successfully!\n","------------------------------\n","Vocabulary Size: 14\n","Learned Vocabulary (Word -> ID):\n","0: \n","1: [UNK]\n","2: the\n","3: dog\n","4: cat\n","5: sat\n","6: on\n","7: my\n","8: mat\n","9: homework\n","------------------------------\n","Original sentence: 'the dog and cat are good friends'\n","Encoded sequence: [[ 2  3 13  4 12  1 10  0]]\n","------------------------------\n","Original sentence: 'the cat sat on the mat'\n","Encoded sequence: [[2 4 5 6 2 8 0 0]]\n","------------------------------\n","Original sentence: 'the dog ate my homework'\n","Encoded sequence: [[ 2  3 11  7  9  0  0  0]]\n","------------------------------\n","Original sentence: 'the cat and the dog are friends'\n","Encoded sequence: [[ 2  4 13  2  3 12 10  0]]\n","------------------------------\n","Sequence to decode: [ 2  4 13  2  3 12 10  0]\n","Decoded sentence: 'the cat and the dog are friends'\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","# --- Step 1: Gather and Prepare a Corpus ---\n","# For this demo, our \"corpus\" is just a small list of sentences.\n","# In a real project, this would be thousands or millions of sentences from your dataset.\n","corpus = [\n","    \"the cat sat on the mat\",\n","    \"the dog ate my homework\",\n","    \"the cat and the dog are friends\"\n","]\n","\n","# --- Step 2 & 3: Choose a Strategy and Build the Vocabulary ---\n","# We will use a simple word-based strategy. The TextVectorization layer is a\n","# convenient tool that handles the entire vocabulary creation process.\n","\n","# Define the maximum number of words to include in the vocabulary.\n","# The layer will automatically pick the most frequent words.\n","vocab_size = 15\n","\n","# Create the TextVectorization layer. This is our tokenizer.\n","# It will handle normalization (like lowercasing) and splitting text into words.\n","# `output_sequence_length` pads or truncates sentences to a fixed length.\n","text_vectorizer = tf.keras.layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_sequence_length=8\n",")\n","\n","# Train the tokenizer on our corpus to build the vocabulary.\n","# The .adapt() method reads the corpus, counts word frequencies,\n","# and creates the mapping from words to integer IDs.\n","print(\"--- Training the tokenizer... ---\")\n","text_vectorizer.adapt(corpus)\n","print(\"Vocabulary built successfully!\")\n","print(\"-\" * 30)\n","\n","\n","# --- Step 4: Inspect the Vocabulary (and Special Tokens) ---\n","# We can now view the vocabulary that the layer has learned.\n","# The layer automatically handles adding special tokens like [UNK] for\n","# out-of-vocabulary words and '0' for padding.\n","vocabulary = text_vectorizer.get_vocabulary()\n","print(f\"Vocabulary Size: {len(vocabulary)}\")\n","print(\"Learned Vocabulary (Word -> ID):\")\n","# Print the first 10 words and their corresponding IDs (indices).\n","for i, word in enumerate(vocabulary[:10]):\n","    print(f\"{i}: {word}\")\n","print(\"-\" * 30)\n","\n","\n","# --- Step 5: Finalize and Use the Tokenizer ---\n","# Our tokenizer is now ready to use!\n","\n","# --- Demonstration: Encoding (Text to Numbers) ---\n","sentence_to_encode = \"the dog and cat are good friends\"\n","print(f\"Original sentence: '{sentence_to_encode}'\")\n","\n","# Use the trained tokenizer to convert the sentence into a sequence of integer IDs.\n","# Note that 'good' is not in our original corpus, so it will be mapped to the [UNK] token (ID 1).\n","encoded_sentence = text_vectorizer([sentence_to_encode])\n","print(f\"Encoded sequence: {encoded_sentence.numpy()}\")\n","print(\"-\" * 30)\n","\n","\n","sentence_to_encode = \"the cat sat on the mat\"\n","print(f\"Original sentence: '{sentence_to_encode}'\")\n","\n","# Use the trained tokenizer to convert the sentence into a sequence of integer IDs.\n","# Note that 'good' is not in our original corpus, so it will be mapped to the [UNK] token (ID 1).\n","encoded_sentence = text_vectorizer([sentence_to_encode])\n","print(f\"Encoded sequence: {encoded_sentence.numpy()}\")\n","print(\"-\" * 30)\n","\n","\n","sentence_to_encode = \"the dog ate my homework\"\n","print(f\"Original sentence: '{sentence_to_encode}'\")\n","\n","# Use the trained tokenizer to convert the sentence into a sequence of integer IDs.\n","# Note that 'good' is not in our original corpus, so it will be mapped to the [UNK] token (ID 1).\n","encoded_sentence = text_vectorizer([sentence_to_encode])\n","print(f\"Encoded sequence: {encoded_sentence.numpy()}\")\n","print(\"-\" * 30)\n","\n","\n","sentence_to_encode = \"the cat and the dog are friends\"\n","print(f\"Original sentence: '{sentence_to_encode}'\")\n","\n","# Use the trained tokenizer to convert the sentence into a sequence of integer IDs.\n","# Note that 'good' is not in our original corpus, so it will be mapped to the [UNK] token (ID 1).\n","encoded_sentence = text_vectorizer([sentence_to_encode])\n","print(f\"Encoded sequence: {encoded_sentence.numpy()}\")\n","print(\"-\" * 30)\n","\n","\n","\n","# --- Demonstration: Decoding (Numbers to Text) ---\n","# We can also build a simple decoder to convert the numbers back to text.\n","# First, create a reverse mapping from ID to word.\n","id_to_word_map = {i: word for i, word in enumerate(vocabulary)}\n","\n","encoded_sequence_to_decode = encoded_sentence.numpy()[0]\n","print(f\"Sequence to decode: {encoded_sequence_to_decode}\")\n","\n","# Decode the sequence by looking up each ID in our map.\n","# We'll ignore padding tokens (ID 0).\n","decoded_sentence = ' '.join(id_to_word_map[i] for i in encoded_sequence_to_decode if i > 0)\n","print(f\"Decoded sentence: '{decoded_sentence}'\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"37944a60","metadata":{"id":"37944a60"},"outputs":[],"source":["'''\n","Obtaining word embeddings using the Keras `Embedding` layer is like using a **smart dictionary**.\n","Instead of looking up a word to get its definition, look up a word's unique ID number to get a dense vector of\n","numbers that represents its meaning.\n","\n","The `Embedding` layer is essentially a **lookup table** that you create and train.\n","It stores one vector for every word in your vocabulary.\n","\n","The Three-Step Process\n","\n","1. Step 1: Prepare Your Data (Text to Integers)\n","\n","First, convert raw text into sequences of integer IDs. Each unique word in your entire dataset is assigned\n","a unique integer.\n","\n","For example, the sentence:\n","`\"the cat sat on the mat\"`\n","\n","Becomes a sequence of integers:\n","`[2 4 5 6 2 8 0 0]`\n","\n","2. Step 2: Define the `Embedding` Layer üìñ\n","\n","When you create the layer, you define two key parameters:\n","\n","1.  input_dim: This is the size of your vocabulary (the total number of unique words). In our example, it would be 14 (12 words + 1 for a '0' padding token + 1 for [UNK]).\n","2.  output_dim: This is the size of the dense vector you want for each word. This is a hyperparameter you choose. A common size is 128, 256, or 512.\n","\n","\n","```python\n","# Let's say we want a 20-dimensional vector for each word.\n","embedding_layer = tf.keras.layers.Embedding(input_dim=14, output_dim=20)\n","```\n","\n","Behind the scenes, Keras creates a simple but powerful weight matrix (our \"lookup table\") of shape `(input_dim, output_dim)`.\n","For our example, this would be a (14, 20) matrix. Initially, this matrix is filled with small random numbers, then gets adjusted during training.\n","\n","3. Step 3: The Lookup Operation üîç\n","\n","When you pass your integer sequence [2 4 5 6 2 8 0 0] into the layer, it performs a direct lookup.\n","\n","  * For the integer 2, it grabs the corresponding vector of the matrix.\n","  * For the integer 4, it grabs the corresponding vector of the matrix.\n","  * And so on...\n","\n","The output is a new sequence where each integer ID has been replaced by its corresponding dense vector from the lookup table.\n","\n","The Magic: The most important part is that these vectors are LEARNED during training.\n","Through backpropagation, the model adjusts the values in these vectors. As a result, words that are used in similar contexts\n","(e.g., \"cat\" and \"dog,\" or \"king\" and \"queen\") will end up having similar-looking vectors.\n","This is how the model captures the semantic meaning of words.\n","\n","\n","'''"]},{"cell_type":"code","execution_count":null,"id":"40c3a58f","metadata":{"id":"40c3a58f","outputId":"8775c9bd-700d-42ea-8fe5-5077898963f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x771c5c278790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","--- Model Architecture ---\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," text_vectorization_4 (TextV  (None, 8)                0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_layer (Embedding)  (None, 8, 20)            280       \n","                                                                 \n"," global_average_pooling1d (G  (None, 20)               0         \n"," lobalAveragePooling1D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1)                 21        \n","                                                                 \n","=================================================================\n","Total params: 301\n","Trainable params: 301\n","Non-trainable params: 0\n","_________________________________________________________________\n","------------------------------\n","\n","--- Initial Embedding Weights (before training) ---\n","Shape of embedding matrix: (14, 20)\n","(Rows = Vocabulary Size, Columns = Embedding Dimension)\n","\n","Embedding vector for a specific word:\n","Word: 'dog' (ID: 3)\n","Initial Vector: [-0.01854184 -0.00290833  0.04710161 -0.03249319 -0.01185932  0.04403374\n"," -0.04353477  0.04332656 -0.04766128  0.01441381 -0.04951577  0.01788727\n","  0.01891855 -0.01470874  0.01410434 -0.04563617 -0.01856861  0.01068039\n","  0.02948295  0.04588601]\n","\n","After training on a real task, this vector would capture the 'meaning' of the word 'dog'.\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","# --- Step 1: Reuse the Tokenizer Developed Previously ---\n","# We'll start with the same corpus and the trained TextVectorization layer\n","# from the previous example.\n","\n","corpus = [\n","    \"the cat sat on the mat\",\n","    \"the dog ate my homework\",\n","    \"the cat and the dog are friends\"\n","]\n","\n","vocab_size = 15\n","text_vectorizer = tf.keras.layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_sequence_length=8\n",")\n","# Train the tokenizer on our corpus to build the vocabulary.\n","text_vectorizer.adapt(corpus)\n","\n","\n","# --- Step 2: Build a Model to Learn Embeddings ---\n","# We'll create a simple Keras model. The key is that the first layer is our\n","# tokenizer, and the second is the Embedding layer.\n","\n","# Define the dimensionality of the word embeddings we want to learn.\n","embedding_dim = 20\n","\n","model = tf.keras.Sequential([\n","    # 1. The TextVectorization layer: This layer takes raw text strings as input\n","    #    and outputs integer sequences. It's the bridge from text to numbers.\n","    text_vectorizer,\n","\n","    # 2. The Embedding layer: This layer takes the integer sequences and looks up\n","    #    the corresponding embedding vector for each token. The `input_dim` must\n","    #    match the vocabulary size from our tokenizer.\n","    tf.keras.layers.Embedding(\n","        input_dim=len(text_vectorizer.get_vocabulary()),\n","        output_dim=embedding_dim,\n","        name=\"embedding_layer\" # Give the layer a name to easily access it later\n","    ),\n","\n","    # 3. A Pooling layer: To get a single vector representation for the whole\n","    #    sentence, we average the embeddings of all words in the sequence.\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","\n","    # 4. A final Dense layer: To make this a trainable model, we add a simple\n","    #    output layer. For a real task, this would be your classification or\n","    #    regression output.\n","    tf.keras.layers.Dense(1, activation='sigmoid') # Example for binary classification\n","])\n","\n","# Compile the model to prepare it for training\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Print the model summary to see the architecture\n","print(\"--- Model Architecture ---\")\n","model.summary()\n","print(\"-\" * 30)\n","\n","\n","# --- Step 3: Inspect the Learned Embeddings ---\n","# Although we haven't trained the model on any data yet, the embedding layer\n","# already has a randomly initialized weight matrix. After training, these weights\n","# would contain the meaningful learned embeddings.\n","\n","# Get the embedding layer from the model by its name\n","embedding_layer = model.get_layer('embedding_layer')\n","\n","# The weights are a list, where the first element is the embedding matrix\n","embedding_weights = embedding_layer.get_weights()[0]\n","\n","# Get the vocabulary from our tokenizer\n","vocabulary = text_vectorizer.get_vocabulary()\n","\n","print(\"\\n--- Initial Embedding Weights (before training) ---\")\n","print(f\"Shape of embedding matrix: {embedding_weights.shape}\")\n","print(\"(Rows = Vocabulary Size, Columns = Embedding Dimension)\")\n","print(\"\\nEmbedding vector for a specific word:\")\n","# Let's find the ID for the word 'dog'\n","word_to_find = 'dog'\n","word_id = vocabulary.index(word_to_find)\n","# The embedding vector is the row in the weight matrix corresponding to the word's ID\n","word_vector = embedding_weights[word_id]\n","print(f\"Word: '{word_to_find}' (ID: {word_id})\")\n","print(f\"Initial Vector: {word_vector}\")\n","print(\"\\nAfter training on a real task, this vector would capture the 'meaning' of the word 'dog'.\")\n","\n"]}],"metadata":{"kernelspec":{"display_name":"transformer-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}